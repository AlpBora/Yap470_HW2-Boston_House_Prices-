{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, roc_auc_score, r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv(\"../input/boston-house-prices/housing.csv\", delim_whitespace=True, names=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "plt.hist(df[\"MEDV\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "cm = df.corr()\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cm,annot=True,cmap= plt.cm.Reds,fmt= \".0%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "INDUS, RM, TAX, PTRATIO and LSTAT has good corralation with target feature MEDV. But INDUS also has high correlation with LTSAT and TAX. So in this case i should not use INDUS. The best features are RM, TAX, PTRATIO and LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = df[[\"RM\", \"TAX\", \"PTRATIO\",\"LSTAT\", \"MEDV\"]]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(df,test_size,shuffle): # from scratch \n",
    "    if shuffle:\n",
    "        df = df.reindex(np.random.permutation(df.index))\n",
    "    total_sample_size= len(df)\n",
    "    train_end = round(total_sample_size*(1-test_size))\n",
    "    train = df[0:train_end]\n",
    "    test = df[train_end:total_sample_size]\n",
    "    return train,test\n",
    "\n",
    "def cross_validation(df,fold,train_size,feature_count): # from scratch \n",
    "    dataset_size = len(df) \n",
    "     \n",
    "    division_num = 1//(1-train_size)\n",
    "    kfold = np.zeros((dataset_size//fold,feature_count), dtype=int)\n",
    "    val_size = int(dataset_size*(1-train_size))\n",
    "    train_size = int(dataset_size*train_size)\n",
    "    val = np.zeros((val_size,feature_count),dtype=int)\n",
    "    train = np.zeros((train_size,feature_count),dtype=int)\n",
    "    \n",
    "    for i in range(fold):\n",
    "        kfold = np.append(kfold,df[i*(dataset_size//fold):((i+1)*dataset_size//fold)-1].to_numpy(),axis=0)\n",
    "        # first column is zero\n",
    "        val = np.append(val,kfold[int((i+1)*dataset_size//fold):int((i+1)*dataset_size//fold + val_size//fold )-1],axis=0)\n",
    "        train = np.append(train,kfold[int((i+1)*dataset_size//fold + val_size//fold ):int((i+1)*dataset_size//fold + val_size//fold + train_size//fold)-1],axis=0)\n",
    "    val = np.delete(val,np.linspace(0,val_size-1,val_size,dtype=int),0)\n",
    "    train = np.delete(train,np.linspace(0,train_size-1,train_size,dtype=int),0)\n",
    "    return train, val\n",
    "\n",
    "train, test = train_test_split(df,test_size=0.16,shuffle=True)        \n",
    "train, val = cross_validation(train,fold=5,train_size = 0.79,feature_count=5)\n",
    "\n",
    "df_train = pd.DataFrame(train,columns=df.columns)\n",
    "df_val = pd.DataFrame(val,columns=df.columns)\n",
    "df_test = pd.DataFrame(test,columns=df.columns)\n",
    "total_data_size = len(df)\n",
    "\n",
    "\n",
    "print(\"Train ratio: %\",len(train)/total_data_size*100)\n",
    "print(\"Test ratio: %\",len(test)/total_data_size*100)\n",
    "print(\"Validation ratio: %\",len(val)/total_data_size*100)\n",
    "print(\"Train/Test ratio: %\",len(train)/(len(train)+len(test))*100,\"- %\", len(test)/(len(train)+len(test))*100)\n",
    "# requested train/test ratio was %80 - %20\n",
    "\n",
    "x_train, y_train = df_train.drop(columns=\"MEDV\").values.tolist(), df_train.MEDV.values.tolist()\n",
    "x_val, y_val = df_val.drop(columns=\"MEDV\").values.tolist(), df_val.MEDV.values.tolist()\n",
    "x_test, y_test = df_test.drop(columns=\"MEDV\").values.tolist(), df_test.MEDV.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "def r_squared(y_true,y_predicted):\n",
    "    mse_model = np.square(np.subtract(y_true,y_pred)).mean() # variance of model\n",
    "    mse_baseline = np.square(np.subtract(y_pred.mean(),y_pred)).mean() # variance of target variable\n",
    "    r_squared = 1 - (mse_model/mse_baseline)\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        '''        \n",
    "        Initial weights and biases assigned random values ranging '0' to '1'. \n",
    "        We have a total of 9 weights and 4 biases.\n",
    "        6 weights are coming in the hidden layer, two for each neuron, hence 3 x 2 = 6.\n",
    "        The rest of the weights are coming into the output layer.\n",
    "        Same story for the biases. Each bias is attached to the neuron in the hidden layer\n",
    "        and output layer.\n",
    "        \n",
    "        output layer has one neuron because its a regression problem\n",
    "        '''\n",
    "        # Weights\n",
    "        self.w1, \\\n",
    "        self.w2, \\\n",
    "        self.w3, \\\n",
    "        self.w4, \\\n",
    "        self.w5, \\\n",
    "        self.w6, \\\n",
    "        self.w7, \\\n",
    "        self.w8, \\\n",
    "        self.w9, \\\n",
    "        self.w10, \\\n",
    "        self.w11, \\\n",
    "        self.w12, \\\n",
    "        self.w13, \\\n",
    "        self.w14, \\\n",
    "        self.w15, \\\n",
    "        self.w16, \\\n",
    "        self.w17, \\\n",
    "        self.w18, \\\n",
    "        self.w19, \\\n",
    "        self.w20 = np.random.rand(20)\n",
    "\n",
    "        # Biases\n",
    "        self.b_n1, \\\n",
    "        self.b_n2, \\\n",
    "        self.b_n3, \\\n",
    "        self.b_n4, \\\n",
    "        self.b_y_hat = np.random.rand(5) \n",
    "\n",
    "    # Activation function for the neurons\n",
    "    # Each neuron IS an actually activation function itself\n",
    "    # sigmoid is for forward propagation, sigmoid derivative is for back propagation\n",
    "    def sigmoid(self, x): return 1 / (1 + np.e**-x)\n",
    "    def sigmoid_der(self, x): return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Feedforward function produces result of the network prediction for each sample.\n",
    "    First we find neurons' values for hidden layer, then for output layer.\n",
    "    \n",
    "    'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'. \n",
    "    Jeff Heaton, the author of Introduction to Neural Networks in Java, offers a few more.\n",
    "\n",
    "    for most problems, one could probably get decent performance (even without a second optimization step) by setting\n",
    "    the hidden layer configuration using just two rules: (i) the number of hidden layers equals one;\n",
    "    and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers.\n",
    "    \n",
    "    (4 + 1)/2 = 2.5, 3 neuron for hidden layer should be enough\n",
    "    '''\n",
    "    def feedforward(self, x):\n",
    "        \n",
    "        # x[0], x[1] - features\n",
    "        # n* - neurons in the hidden layer, y_hat - predicted value\n",
    "        self.n1    = self.sigmoid(x[0]*self.w1 + x[1]*self.w2 + + x[2]*self.w3 + x[3]*self.w4 + self.b_n1) \n",
    "        self.n2    = self.sigmoid(x[0]*self.w5 + x[1]*self.w6 + + x[2]*self.w7 + x[3]*self.w8 + self.b_n2) \n",
    "        self.n3    = self.sigmoid(x[0]*self.w9 + x[1]*self.w10 + + x[2]*self.w11 + x[3]*self.w12 + self.b_n3)\n",
    "        self.n4    = self.sigmoid(x[0]*self.w13 + x[1]*self.w14 + + x[2]*self.w15 + x[3]*self.w16 + self.b_n4) \n",
    "    \n",
    "        self.y_hat = self.sigmoid(self.n1*self.w17 + self.n2*self.w18 + self.n3*self.w19 + self.n4*self.w20 + self.b_y_hat)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Backpropagation updates all the weights and biases of the network.\n",
    "    By using Gradient Descent technique, each trainable parameter (weight or bias)\n",
    "    is changing a little bit towards minimum of MSE.\n",
    "    Unlike forward propagation, we tweak our parameters starting\n",
    "    from the right end of the network, meaning first we update\n",
    "    weights and biases for output layer, then for the hidden.\n",
    "    If we had more than one hidden layer, we would go over them \n",
    "    is similar fashion, \n",
    "    like this: \"output layer\" => \"hidden layer 2\" => \"hidden layer 1\"\n",
    "    '''\n",
    "    def backpropagation(self, x, y):\n",
    "\n",
    "        # We calculate some values here to use them later\n",
    "        y_hat_der = (-2 * (y-self.y_hat) * self.sigmoid_der(self.n1*self.w13 + self.n2*self.w14 + self.n3*self.w15  + self.b_y_hat))\n",
    "        n1_der = self.w17 * self.sigmoid_der(x[0]*self.w1 + x[1]*self.w2 + + x[2]*self.w3 + x[3]*self.w4 + self.b_n1)\n",
    "        n2_der = self.w18 * self.sigmoid_der(x[0]*self.w5 + x[1]*self.w6 + + x[2]*self.w7 + x[3]*self.w8 + self.b_n2)\n",
    "        n3_der = self.w19 * self.sigmoid_der(x[0]*self.w9 + x[1]*self.w10 + + x[2]*self.w11 + x[3]*self.w12 + self.b_n3)\n",
    "        n4_der = self.w20 * self.sigmoid_der(x[0]*self.w13 + x[1]*self.w14 + + x[2]*self.w15 + x[3]*self.w16 + self.b_n4)\n",
    "        \n",
    "        # Biases\n",
    "        self.b_n1    -= self.lr * y_hat_der * n1_der\n",
    "        self.b_n2    -= self.lr * y_hat_der * n2_der\n",
    "        self.b_n3    -= self.lr * y_hat_der * n3_der\n",
    "        self.b_n4    -= self.lr * y_hat_der * n4_der\n",
    "        self.b_y_hat -= self.lr * y_hat_der\n",
    "\n",
    "        # Weights\n",
    "        self.w17 -= self.lr * y_hat_der * self.n1\n",
    "        self.w18 -= self.lr * y_hat_der * self.n2\n",
    "        self.w19 -= self.lr * y_hat_der * self.n3\n",
    "        self.w20 -= self.lr * y_hat_der * self.n4\n",
    "        \n",
    "        self.w1 -= self.lr * y_hat_der * n1_der * x[0]\n",
    "        self.w2 -= self.lr * y_hat_der * n1_der * x[1]\n",
    "        self.w3 -= self.lr * y_hat_der * n1_der * x[2]\n",
    "        self.w4 -= self.lr * y_hat_der * n1_der * x[3]\n",
    "        self.w5 -= self.lr * y_hat_der * n2_der * x[0]\n",
    "        self.w6 -= self.lr * y_hat_der * n2_der * x[1]\n",
    "        self.w7 -= self.lr * y_hat_der * n2_der * x[2]\n",
    "        self.w8 -= self.lr * y_hat_der * n2_der * x[3]\n",
    "        self.w9 -= self.lr * y_hat_der * n3_der * x[0]\n",
    "        self.w10 -= self.lr * y_hat_der * n3_der * x[1]\n",
    "        self.w11 -= self.lr * y_hat_der * n3_der * x[2]\n",
    "        self.w12 -= self.lr * y_hat_der * n3_der * x[3]\n",
    "        self.w13 -= self.lr * y_hat_der * n4_der * x[0]\n",
    "        self.w14 -= self.lr * y_hat_der * n4_der * x[1]\n",
    "        self.w15 -= self.lr * y_hat_der * n4_der * x[2]\n",
    "        self.w16 -= self.lr * y_hat_der * n4_der * x[3]\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Training process is the combination of forward and back propagations.\n",
    "    '''\n",
    "    def fit(self, X, y, epoch=10, lr=0.01):\n",
    "\n",
    "        mse_list = []\n",
    "        self.lr = lr\n",
    "\n",
    "        # Loop to go over epochs. Each epoch train network on all available data.\n",
    "        # We also check MSE and store it to visualize training process\n",
    "        for i in range(epoch):\n",
    "            mse = mean_squared_error(y, self.predict(X))\n",
    "            mse_list.append(mse)\n",
    "            print(f'Epoch: {i+1} / {epoch}, MSE: {round(mse, 4)}', end='\\r')\n",
    "\n",
    "            # Loop to go over each training example for current epoch\n",
    "            for j in range(len(X)):\n",
    "                self.feedforward(X[j])\n",
    "                self.backpropagation(X[j], y[j]) \n",
    "\n",
    "        return mse_list\n",
    "\n",
    "    '''\n",
    "    This function is very similar to feed forward,\n",
    "    it's in fact uses 'feedforward' function to make predictions.\n",
    "    The only difference is that we predict outcome for all samples.\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for x in X:\n",
    "            self.feedforward(x)\n",
    "            result.append(self.y_hat)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "R-square : R2=1−MSE(model)MSE(baseline)=1−∑Ni=1(y1−y1^)2∑Ni=1(y1¯−y1^)2\n",
    "adjusted R-square: R2¯=1−(1−R2)(n−1n−k+1)\n",
    "                    k: number of features\n",
    "                    n: number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "model = SimpleNeuralNetwork()\n",
    "lr_list = [0.001,0.005, 0.01, 0.05]\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.title('Neural Network Training Process', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE', fontsize=12)\n",
    "\n",
    "for lr in lr_list:\n",
    "    history = model.fit(x_train, y_train, epoch=100, lr=lr)\n",
    "    plt.plot(history,label=f\"Learning rate: {lr}\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# keras_model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "history = model.fit(x_train, y_train, epoch=100, lr=0.05)\n",
    "val_pred = model.predict(x_val)\n",
    "train_pred = model.predict(x_train)\n",
    "test_pred = model.predict(x_test)\n",
    "\n",
    "val_acc_score =r2_score(y_val, val_pred)\n",
    "train_acc_score =r2_score(y_train, train_pred)\n",
    "test_acc_score =r2_score(y_test, test_pred)\n",
    "\n",
    "# TODO build other neurons for other 12 features\n",
    "\n",
    "print(f\"Validation Accuracy Score:\\t {val_acc_score, r_squared(y_val,val_pred)} \")\n",
    "print(f\"Training Accuracy Score:\\t {train_acc_score, r_squared(y_train, train_pred)} \")\n",
    "print(f\"Test Accuracy Score:\\t\\t {test_acc_score, r_squared(y_test, test_pred)} \")\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.scatter(x=y_val,y=val_pred,alpha=0.5)\n",
    "plt.xlabel('y_test',size=12)\n",
    "plt.ylabel('y_pred',size=12)\n",
    "plt.title('Predicited Values vs Original Values (Test Set)',size=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
